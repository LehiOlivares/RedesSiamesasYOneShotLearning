{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c3b1936f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import random\n",
    "from keras import losses, optimizers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e96e81a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar datos desde CSV\n",
    "data = pd.read_csv('C:/Users/LEHI/Documents/Practicas profesionales/ABRIL/preprocessed data 18 columnas - etiqueta letras.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "bb816634",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_uniq = data.groupby('clase').head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f680df6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_uniq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "89cf4626",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndef one_shot_data(dataset_path):\\n    # Cargar el dataset\\n    dataset = pd.read_csv(dataset_path)\\n    \\n    # Inicializar un diccionario para almacenar un solo ejemplo por clase\\n    one_shot_data = {}\\n    \\n    # Obtener las clases únicas del dataset\\n    classes = dataset['clase'].unique()\\n    \\n    # Seleccionar un solo ejemplo por clase\\n    for cls in classes:\\n        class_data = dataset.loc[dataset['clase'] == cls].iloc[0].drop('clase')\\n        one_shot_data[cls] = class_data.values  # Convertir a un array de numpy\\n    return one_shot_data\\n\""
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "def one_shot_data(dataset_path):\n",
    "    # Cargar el dataset\n",
    "    dataset = pd.read_csv(dataset_path)\n",
    "    \n",
    "    # Inicializar un diccionario para almacenar un solo ejemplo por clase\n",
    "    one_shot_data = {}\n",
    "    \n",
    "    # Obtener las clases únicas del dataset\n",
    "    classes = dataset['clase'].unique()\n",
    "    \n",
    "    # Seleccionar un solo ejemplo por clase\n",
    "    for cls in classes:\n",
    "        class_data = dataset.loc[dataset['clase'] == cls].iloc[0].drop('clase')\n",
    "        one_shot_data[cls] = class_data.values  # Convertir a un array de numpy\n",
    "    return one_shot_data\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1c5020ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir los datos en características (X) y etiquetas (y)\n",
    "X = df_uniq.drop('clase', axis=1)\n",
    "y = df_uniq['clase']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9afc0862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir etiquetas a números\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2e97f656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir datos en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalizar los datos (opcional pero recomendado)\n",
    "X_train = (X_train - X_train.mean()) / X_train.std()\n",
    "X_test = (X_test - X_test.mean()) / X_test.std()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b6428d65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Preparar pares de datos para one-shot learning\\ndef create_pairs(X, digit_indices):\\n    pairs = []\\n    labels = []\\n    n = min([len(digit_indices[d]) for d in range(12)]) - 1\\n    for d in range(12):\\n        for i in range(n):\\n            z1, z2 = digit_indices[d][i], digit_indices[d][i + 1]\\n            pairs += [[X.iloc[z1].values, X.iloc[z2].values]]\\n            labels += [1 if d == i else 0 for i in range(12)]\\n    return np.array(pairs), np.array(labels)\\n'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Preparar pares de datos para one-shot learning\n",
    "def create_pairs(X, digit_indices):\n",
    "    pairs = []\n",
    "    labels = []\n",
    "    n = min([len(digit_indices[d]) for d in range(12)]) - 1\n",
    "    for d in range(12):\n",
    "        for i in range(n):\n",
    "            z1, z2 = digit_indices[d][i], digit_indices[d][i + 1]\n",
    "            pairs += [[X.iloc[z1].values, X.iloc[z2].values]]\n",
    "            labels += [1 if d == i else 0 for i in range(12)]\n",
    "    return np.array(pairs), np.array(labels)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "56a6a332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir la arquitectura de la red siamesa\n",
    "input_shape = X_train.shape[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9201da11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir función para crear la rama de la red siamesa\n",
    "def create_siamese_branch(input_shape):\n",
    "    input_layer = tf.keras.layers.Input(shape=(input_shape,))\n",
    "    x = tf.keras.layers.Dense(64, activation='relu')(input_layer)\n",
    "    x = tf.keras.layers.Dropout(0.2)(x)\n",
    "    x = tf.keras.layers.Dense(32, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dropout(0.2)(x)\n",
    "    return tf.keras.Model(inputs=input_layer, outputs=x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "88997ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear la rama izquierda y derecha de la red siamesa\n",
    "left_input = tf.keras.layers.Input(shape=(input_shape,))\n",
    "right_input = tf.keras.layers.Input(shape=(input_shape,))\n",
    "\n",
    "left_branch = create_siamese_branch(input_shape)(left_input)\n",
    "right_branch = create_siamese_branch(input_shape)(right_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9fac53bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenar las salidas de ambas ramas\n",
    "merged = tf.keras.layers.Concatenate()([left_branch, right_branch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "507245f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agregar capas adicionales y la capa de salida\n",
    "x = tf.keras.layers.Dense(32, activation='relu')(merged)\n",
    "x = tf.keras.layers.Dropout(0.2)(x)\n",
    "output = tf.keras.layers.Dense(12, activation='softmax')(x)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c618eac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construir el modelo\n",
    "model = tf.keras.Model(inputs=[left_input, right_input], outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b8703a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compilar el modelo\n",
    "model.compile(optimizer='adam', loss=losses.SparseCategoricalCrossentropy(), metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1fd53350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "10/10 [==============================] - 1s 29ms/step - loss: 2.4600 - accuracy: 0.1373 - val_loss: 2.4326 - val_accuracy: 0.2051\n",
      "Epoch 2/50\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 2.3191 - accuracy: 0.2418 - val_loss: 2.2717 - val_accuracy: 0.3846\n",
      "Epoch 3/50\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 2.1486 - accuracy: 0.3464 - val_loss: 2.1265 - val_accuracy: 0.4615\n",
      "Epoch 4/50\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 2.0036 - accuracy: 0.4248 - val_loss: 1.9843 - val_accuracy: 0.4872\n",
      "Epoch 5/50\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 1.8393 - accuracy: 0.4771 - val_loss: 1.8358 - val_accuracy: 0.5385\n",
      "Epoch 6/50\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 1.7118 - accuracy: 0.5686 - val_loss: 1.6996 - val_accuracy: 0.5897\n",
      "Epoch 7/50\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 1.6213 - accuracy: 0.5752 - val_loss: 1.5637 - val_accuracy: 0.6154\n",
      "Epoch 8/50\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 1.3768 - accuracy: 0.6797 - val_loss: 1.4314 - val_accuracy: 0.6410\n",
      "Epoch 9/50\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 1.2873 - accuracy: 0.6667 - val_loss: 1.2919 - val_accuracy: 0.7436\n",
      "Epoch 10/50\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 1.1768 - accuracy: 0.6209 - val_loss: 1.1749 - val_accuracy: 0.8205\n",
      "Epoch 11/50\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 1.0264 - accuracy: 0.7451 - val_loss: 1.0960 - val_accuracy: 0.8205\n",
      "Epoch 12/50\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.9169 - accuracy: 0.7255 - val_loss: 1.0253 - val_accuracy: 0.8205\n",
      "Epoch 13/50\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.8561 - accuracy: 0.7647 - val_loss: 0.9336 - val_accuracy: 0.8462\n",
      "Epoch 14/50\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.8335 - accuracy: 0.7712 - val_loss: 0.8787 - val_accuracy: 0.8205\n",
      "Epoch 15/50\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.7432 - accuracy: 0.7778 - val_loss: 0.8083 - val_accuracy: 0.8462\n",
      "Epoch 16/50\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.6923 - accuracy: 0.8301 - val_loss: 0.7720 - val_accuracy: 0.8205\n",
      "Epoch 17/50\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.6559 - accuracy: 0.8170 - val_loss: 0.7695 - val_accuracy: 0.8205\n",
      "Epoch 18/50\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.5870 - accuracy: 0.8366 - val_loss: 0.7537 - val_accuracy: 0.8205\n",
      "Epoch 19/50\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.4688 - accuracy: 0.8889 - val_loss: 0.7290 - val_accuracy: 0.8462\n",
      "Epoch 20/50\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.4663 - accuracy: 0.8824 - val_loss: 0.7090 - val_accuracy: 0.8462\n",
      "Epoch 21/50\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.5584 - accuracy: 0.8431 - val_loss: 0.7210 - val_accuracy: 0.8205\n",
      "Epoch 22/50\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.3987 - accuracy: 0.8693 - val_loss: 0.7209 - val_accuracy: 0.8462\n",
      "Epoch 23/50\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.4322 - accuracy: 0.8824 - val_loss: 0.7179 - val_accuracy: 0.8718\n",
      "Epoch 24/50\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.3837 - accuracy: 0.9020 - val_loss: 0.7228 - val_accuracy: 0.8462\n",
      "Epoch 25/50\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.3387 - accuracy: 0.9412 - val_loss: 0.7246 - val_accuracy: 0.8462\n",
      "Epoch 26/50\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.3602 - accuracy: 0.9085 - val_loss: 0.7377 - val_accuracy: 0.8462\n",
      "Epoch 27/50\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.3674 - accuracy: 0.8954 - val_loss: 0.7284 - val_accuracy: 0.8462\n",
      "Epoch 28/50\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.4005 - accuracy: 0.8758 - val_loss: 0.7304 - val_accuracy: 0.8462\n",
      "Epoch 29/50\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.2182 - accuracy: 0.9477 - val_loss: 0.7160 - val_accuracy: 0.8462\n",
      "Epoch 30/50\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.2941 - accuracy: 0.9281 - val_loss: 0.7000 - val_accuracy: 0.8718\n",
      "Epoch 31/50\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.2375 - accuracy: 0.9281 - val_loss: 0.7075 - val_accuracy: 0.8718\n",
      "Epoch 32/50\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.2360 - accuracy: 0.9542 - val_loss: 0.6938 - val_accuracy: 0.8718\n",
      "Epoch 33/50\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.2118 - accuracy: 0.9346 - val_loss: 0.6728 - val_accuracy: 0.8718\n",
      "Epoch 34/50\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.2128 - accuracy: 0.9346 - val_loss: 0.6715 - val_accuracy: 0.8718\n",
      "Epoch 35/50\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.2049 - accuracy: 0.9346 - val_loss: 0.6562 - val_accuracy: 0.8974\n",
      "Epoch 36/50\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.2667 - accuracy: 0.9150 - val_loss: 0.6469 - val_accuracy: 0.8718\n",
      "Epoch 37/50\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1962 - accuracy: 0.9346 - val_loss: 0.6445 - val_accuracy: 0.8718\n",
      "Epoch 38/50\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.2177 - accuracy: 0.9281 - val_loss: 0.6191 - val_accuracy: 0.8718\n",
      "Epoch 39/50\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1766 - accuracy: 0.9542 - val_loss: 0.6158 - val_accuracy: 0.8718\n",
      "Epoch 40/50\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1679 - accuracy: 0.9673 - val_loss: 0.6263 - val_accuracy: 0.8718\n",
      "Epoch 41/50\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1644 - accuracy: 0.9412 - val_loss: 0.6328 - val_accuracy: 0.8718\n",
      "Epoch 42/50\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1591 - accuracy: 0.9542 - val_loss: 0.6312 - val_accuracy: 0.8718\n",
      "Epoch 43/50\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1610 - accuracy: 0.9412 - val_loss: 0.6055 - val_accuracy: 0.8718\n",
      "Epoch 44/50\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1579 - accuracy: 0.9608 - val_loss: 0.6116 - val_accuracy: 0.8718\n",
      "Epoch 45/50\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1880 - accuracy: 0.9346 - val_loss: 0.6336 - val_accuracy: 0.8718\n",
      "Epoch 46/50\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1276 - accuracy: 0.9673 - val_loss: 0.6217 - val_accuracy: 0.8718\n",
      "Epoch 47/50\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1380 - accuracy: 0.9608 - val_loss: 0.6244 - val_accuracy: 0.8718\n",
      "Epoch 48/50\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1587 - accuracy: 0.9346 - val_loss: 0.6220 - val_accuracy: 0.8718\n",
      "Epoch 49/50\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1015 - accuracy: 0.9739 - val_loss: 0.6361 - val_accuracy: 0.8718\n",
      "Epoch 50/50\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.1233 - accuracy: 0.9673 - val_loss: 0.6525 - val_accuracy: 0.8718\n"
     ]
    }
   ],
   "source": [
    "# Entrenar el modelo\n",
    "history = model.fit([X_train, X_train], y_train, epochs=50, batch_size=16, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "579c909a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar datos desde CSV\n",
    "dataEvaluar = pd.read_csv('C:/Users/LEHI/Documents/Practicas profesionales/ABRIL/preprocessed data 18 columnas - etiqueta letras.csv')\n",
    "\n",
    "# Dividir los datos en características (X) y etiquetas (y)\n",
    "X = dataEvaluar.drop('clase', axis=1)\n",
    "y = dataEvaluar['clase']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "59ed9f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataEvaluar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "421ef910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir etiquetas a números\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "0bec7a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir datos en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Normalizar los datos (opcional pero recomendado)\n",
    "X_train = (X_train - X_train.mean()) / X_train.std()\n",
    "X_test = (X_test - X_test.mean()) / X_test.std()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "324afb3b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "# Evaluar el modelo\n",
    "y_pred = model.predict([X_test, X_test])\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "4d1f6743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz de Confusión:\n",
      "[[ 7  0  0  0  0  0  0  0  0  1  0  0]\n",
      " [ 0  6  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0 12  0  0  0  1  0  0  0  0  0]\n",
      " [ 0  0  0  9  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  1 13  0  0  0  0  0  0  0]\n",
      " [ 1  0  0  0  0  9  0  1  0  0  0  0]\n",
      " [ 0  0  0  0  0  0 12  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  1  0 14  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0 14  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  4  0  0]\n",
      " [ 0  2  0  0  0  0  0  0  0  0  9  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  7]]\n"
     ]
    }
   ],
   "source": [
    "# Matriz de confusión\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_classes)\n",
    "print(\"Matriz de Confusión:\")\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "9ea3d0f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exactitud (Accuracy):\n",
      " 0.9354838709677419\n",
      "Precisión (Precision) por clase:\n",
      " [0.875      0.75       1.         0.9        1.         0.9\n",
      " 0.92307692 0.93333333 1.         0.8        1.         1.        ]\n",
      "Sensibilidad (Recall) por clase:\n",
      " [0.875      1.         0.92307692 1.         0.92857143 0.81818182\n",
      " 1.         0.93333333 1.         1.         0.81818182 1.        ]\n",
      "Puntuación F1 (F1 Score) por clase:\n",
      " [0.875      0.85714286 0.96       0.94736842 0.96296296 0.85714286\n",
      " 0.96       0.93333333 1.         0.88888889 0.9        1.        ]\n"
     ]
    }
   ],
   "source": [
    "#Metricas\n",
    "\n",
    "with np.errstate(divide='ignore', invalid='ignore'):\n",
    "    # Exactitud (Accuracy)\n",
    "    accuracy = np.sum(np.diag(conf_matrix)) / np.sum(conf_matrix)\n",
    "    print(\"Exactitud (Accuracy):\\n\", accuracy)\n",
    "\n",
    "    # Precisión (Precision)\n",
    "    precision = np.diag(conf_matrix) / np.sum(conf_matrix, axis=0)\n",
    "    precision[np.isnan(precision)] = 0  # Reemplaza NaN con 0\n",
    "    print(\"Precisión (Precision) por clase:\\n\", precision)\n",
    "\n",
    "    # Sensibilidad (Recall)\n",
    "    sensibilidad = np.diag(conf_matrix) / np.sum(conf_matrix, axis=1)\n",
    "    sensibilidad[np.isnan(sensibilidad)] = 0  # Reemplaza NaN con 0\n",
    "    print(\"Sensibilidad (Recall) por clase:\\n\", sensibilidad)\n",
    "\n",
    "    # Puntuación F1 (F1 Score)\n",
    "    f1_score = 2 * (precision * sensibilidad) / (precision + sensibilidad)\n",
    "    f1_score[np.isnan(f1_score)] = 0  # Reemplaza NaN con 0\n",
    "    print(\"Puntuación F1 (F1 Score) por clase:\\n\", f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d0a1dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141701a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
